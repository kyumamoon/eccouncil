{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Web Page Scraping**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Web scraping downloads the HTML website for offline parsing analysis.\n",
    "- Multiple tools are available as well as the online services.\n",
    "- Considerations\n",
    "- Anoymizing through proxy servers.\n",
    "- Changing user input.\n",
    "- Clear cookies to hide activity. No one wants to be watch."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "proxies = {\"http\": \"183.82.116.56:8080\"}\n",
    "headers = {\"user-agent\":\"Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0\"}\n",
    "\n",
    "# Create a request\n",
    "r = requests.get(\"https://www.scrapethissite.com/pages/\", proxies=proxies, headers=headers)\n",
    "\n",
    "# Print returned HTML\n",
    "print(r.text)\n",
    "\n",
    "# Print All Cookies\n",
    "for cookies in r.cookies:\n",
    "    print(cookies)\n",
    "\n",
    "# Try printing cookie specified.\n",
    "try:\n",
    "    print(r.cookies[\"TestingGround\"])\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configurable Scraper**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\"\"\"\n",
    "configurable_scraper.py\n",
    "Purpose: User configurable web scraper\n",
    "Author: Cody Jackson\n",
    "Date: 2/18/2018\n",
    "########################\n",
    "Version 0.1\n",
    "    Initial build\n",
    "\"\"\"\n",
    "from collections import namedtuple\n",
    "from random import randrange\n",
    "import requests\n",
    "\n",
    "http_proxies = [\"183.82.116.56:8080\", \"217.13.102.86:3128\", \"218.255.102.246:8060\"]\n",
    "user_agents = [\"Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0\",\n",
    "               \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 \"\n",
    "               \"Safari/537.36\",\n",
    "               \"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:64.0) Gecko/20100101 Firefox/64.0\"]\n",
    "\n",
    "\n",
    "def random_agent():\n",
    "    \"\"\"Randomize the user-agent provided to the server.\"\"\"\n",
    "    agent_index = randrange(0, len(user_agents))\n",
    "    headers = {\"user-agent\": \"{}\".format(user_agents[agent_index])}\n",
    "\n",
    "    return headers\n",
    "\n",
    "\n",
    "def random_proxy():\n",
    "    \"\"\"Randomize the proxy service that provides the user IP to the server.\"\"\"\n",
    "    proxy_index = randrange(0, len(http_proxies))\n",
    "    proxy = {\"http\": \"{}\".format(http_proxies[proxy_index])}\n",
    "\n",
    "    return proxy\n",
    "\n",
    "\n",
    "def scraper_request():\n",
    "    \"\"\"Pull HTML and cookies from target.\"\"\"\n",
    "    r = requests.get(\"https://www.scrapethissite.com/pages/\", proxies=random_proxy(), headers=random_agent())\n",
    "\n",
    "    return r.text, r.cookies\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_results = scraper_request()\n",
    "\n",
    "    Scraper = namedtuple(\"Scraper\", [\"html\", \"cookies\"])\n",
    "    scrape = Scraper(scrape_results[0], scrape_results[1])\n",
    "\n",
    "    print(scrape.html)\n",
    "    print(scrape.cookies)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parsing HTML Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/python\n",
    "\"\"\"\n",
    "configurable_scraper.py\n",
    "Purpose: User configurable web scraper\n",
    "Author: Cody Jackson\n",
    "Date: 2/18/2018\n",
    "########################\n",
    "Version 0.1\n",
    "    Initial build\n",
    "\"\"\"\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # file cannot have an extension\n",
    "    with open(\"table_report\") as html_file:\n",
    "        soup = BeautifulSoup(html_file, \"lxml\")\n",
    "\n",
    "    print(soup.title)\n",
    "    print(soup.title.string)\n",
    "    for cell in soup.find_all(\"td\"):\n",
    "        print(cell)\n",
    "    print(soup.prettify())\n",
    "    print(soup.get_text())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
